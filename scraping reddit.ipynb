{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep, perf_counter as pf\n",
    "import re\n",
    "\n",
    "def raze_list(l):\n",
    "    out = []\n",
    "    for x in l:\n",
    "        if type(x) == list:\n",
    "            out += raze_list(x)\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we begin at the most recent Reddit [r/datascience](https://old.reddit.com/r/datascience) subreddit weekly discussion on entering/transitioning in the field.  We're looking for keywords mentioned in the discussion to give an indication of the languages and technologies that data scientists know and use.  We know that Reddit's [demographic](https://old.reddit.com/r/dataisbeautiful/comments/5700sj/octhe_results_of_the_reddit_demographics_survey/) skews towards North American males under 30, so it should be somewhat representative of the kinds of recruits ISED is targeting (with the exception of the male bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_link = 'https://old.reddit.com/r/datascience/comments/9meyte/weekly_entering_transitioning_thread_questions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criteria functions for finding elements in the webpages\n",
    "def post(tag):\n",
    "    return tag.name == 'div' and tag.has_attr('class') and 'expando' in tag['class']\n",
    "def link_to_follow(tag):\n",
    "    return tag.name == 'a' and tag.has_attr('href')\n",
    "\n",
    "def is_comment(tag):\n",
    "    return tag.name == 'div' and tag.has_attr('class') and max(['entry' in x for x in tag['class']])\n",
    "def comment_text(tag):\n",
    "    return tag.name == 'p' and not tag.has_attr('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the text from all the reddit discussions\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}\n",
    "i = 0\n",
    "link = start_link\n",
    "text = ''\n",
    "while link is not None and i < 50:\n",
    "    #print(i, end=', ')\n",
    "    #print(link)\n",
    "    r = requests.get(link, headers = header)\n",
    "    t1 = pf()\n",
    "    page = BeautifulSoup(r.text, 'html.parser')\n",
    "    p = raze_list([list(x.find_all(comment_text)) for x in page.find_all(is_comment)])\n",
    "    text += '\\n'.join(raze_list([list(x.find_all(text=True)) for x in p]))\n",
    "    next_link = page.find(post)\n",
    "    if next_link is not None:\n",
    "        next_link = next_link.find(link_to_follow)\n",
    "        if next_link is not None:\n",
    "            link = next_link['href'].replace('www', 'old')\n",
    "        else:\n",
    "            link = next_link\n",
    "    else:\n",
    "        link = next_link\n",
    "    i += 1\n",
    "    sleep(max(2 - pf() + t1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 50 weekly discussions parsed, these are the results for the frequency of the language/technology keywords found in these discussions, as well as the regular expressions they matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': (re.compile(r'python', re.UNICODE), 613),\n",
       " 'r': (re.compile(r'\\br\\b', re.UNICODE), 466),\n",
       " 'sql': (re.compile(r'sql', re.UNICODE), 327),\n",
       " 'sas': (re.compile(r'\\bsas\\b|statistical analysis system', re.UNICODE), 36),\n",
       " 'excel': (re.compile(r'\\bexcel\\b', re.UNICODE), 130),\n",
       " 'spss': (re.compile(r'spss|statistical package for (the ){0,1}social science',\n",
       "  re.UNICODE),\n",
       "  10),\n",
       " 'hadoop': (re.compile(r'hadoop', re.UNICODE), 21),\n",
       " 'kibana': (re.compile(r'kibana', re.UNICODE), 0),\n",
       " 'tableau': (re.compile(r'tableau', re.UNICODE), 63),\n",
       " 'pig': (re.compile(r'\\bpig\\b', re.UNICODE), 1),\n",
       " 'stata': (re.compile(r'\\bstata\\b', re.UNICODE), 7),\n",
       " 'powerbi': (re.compile(r'power bi', re.UNICODE), 11),\n",
       " 'java': (re.compile(r'java', re.UNICODE), 71),\n",
       " 'c/c++': (re.compile(r'\\bc(\\+\\+){0,1}\\b', re.UNICODE), 72),\n",
       " 'hive': (re.compile(r'\\bhive\\b', re.UNICODE), 5),\n",
       " 'matlab': (re.compile(r'matlab', re.UNICODE), 32),\n",
       " 'ruby': (re.compile(r'ruby', re.UNICODE), 1),\n",
       " 'perl': (re.compile(r'perl', re.UNICODE), 20),\n",
       " 'hbase': (re.compile(r'hbase', re.UNICODE), 0),\n",
       " 'spark': (re.compile(r'spark', re.UNICODE), 26),\n",
       " 'php': (re.compile(r'\\bphp\\b', re.UNICODE), 7),\n",
       " 'scala': (re.compile(r'\\bscala\\b', re.UNICODE), 5),\n",
       " 'tensorflow': (re.compile(r'tensor( ){0,1}flow', re.UNICODE), 16),\n",
       " 'pytorch': (re.compile(r'py( ){0,1}torch', re.UNICODE), 0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = {'python':re.compile('python'), 'r': re.compile('\\\\br\\\\b'), 'sql':re.compile('sql'),\\\n",
    "       'sas':re.compile('\\\\bsas\\\\b|statistical analysis system'), 'excel': re.compile('\\\\bexcel\\\\b'), \\\n",
    "        'spss':re.compile('spss|statistical package for (the ){0,1}social science'), \\\n",
    "       'hadoop':re.compile('hadoop'), 'kibana':re.compile('kibana'), 'tableau':re.compile('tableau'), \\\n",
    "       'pig':re.compile('\\\\bpig\\\\b'), 'stata':re.compile('\\\\bstata\\\\b'), 'powerbi':re.compile('power bi'), \\\n",
    "       'java':re.compile('java'), 'c/c++':re.compile('\\\\bc(\\\\+\\\\+){0,1}\\\\b'), 'hive':re.compile('\\\\bhive\\\\b'), \\\n",
    "       'matlab':re.compile('matlab'), 'ruby':re.compile('ruby'), 'perl':re.compile('perl'), 'hbase':re.compile('hbase'), \\\n",
    "       'spark':re.compile('spark'), 'php':re.compile('\\\\bphp\\\\b'), 'scala':re.compile('\\\\bscala\\\\b'), \\\n",
    "        'tensorflow':re.compile('tensor( ){0,1}flow'), 'pytorch':re.compile('py( ){0,1}torch')}\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "for key in lang:\n",
    "    lang[key] = (lang[key], len(re.findall(lang[key], text)))\n",
    "    \n",
    "lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 50 weekly discussions parsed, these are the results for the frequency of the social media and employment website names found in these discussions, as well as the regular expressions they matched.  It is interesting to note that [Indeed](https://www.indeed.ca/) seems to be the most popularly mentioned employment site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linkedin': (re.compile(r'linked( ){0,1}in', re.UNICODE), 45),\n",
       " 'facebook': (re.compile(r'facebook', re.UNICODE), 11),\n",
       " 'indeed': (re.compile(r'indeed', re.UNICODE), 12),\n",
       " 'glassdoor': (re.compile(r'glass( ){0,1}door', re.UNICODE), 6),\n",
       " 'monster': (re.compile(r'monster', re.UNICODE), 1),\n",
       " 'workopolis': (re.compile(r'workopolis', re.UNICODE), 0),\n",
       " 'kijiji': (re.compile(r'kijiji', re.UNICODE), 0),\n",
       " 'craigslist': (re.compile(r\"craig'{0,1}s {0,1}list\", re.UNICODE), 0)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_media = {'linkedin':re.compile('linked( ){0,1}in'), 'facebook':re.compile('facebook'), \\\n",
    "               'indeed':re.compile('indeed'), 'glassdoor':re.compile('glass( ){0,1}door'), \\\n",
    "               'monster':re.compile('monster'), 'workopolis':re.compile('workopolis'), \\\n",
    "               'kijiji':re.compile('kijiji'), 'craigslist':re.compile(\"craig'{0,1}s {0,1}list\")}\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "for key in social_media:\n",
    "    social_media[key] = (social_media[key], len(re.findall(social_media[key], text)))\n",
    "    \n",
    "social_media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lang = [(key,lang[key][1]) for key in lang]\n",
    "out_sm = [(key,social_media[key][1]) for key in social_media]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_lang = pd.DataFrame(out_lang, columns = ['lang', 'freq'])\n",
    "out_lang.to_csv('reddit_lang.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sm = pd.DataFrame(out_sm, columns = ['site', 'freq'])\n",
    "out_sm.to_csv('reddit_sm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
